{"ast":null,"code":"import _regeneratorRuntime from \"/Users/victordjasliphisitkul/Desktop/Project Official Cov/object-detection-react-victor/node_modules/@babel/runtime/regenerator\";\nimport _slicedToArray from \"/Users/victordjasliphisitkul/Desktop/Project Official Cov/object-detection-react-victor/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\nimport _asyncToGenerator from \"/Users/victordjasliphisitkul/Desktop/Project Official Cov/object-detection-react-victor/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\nimport * as tf from '@tensorflow/tfjs';\n\nconst calculateMaxScores = (scores, numBoxes, numClasses) => {\n  const maxes = [];\n  const classes = [];\n\n  for (let i = 0; i < numBoxes; i++) {\n    let max = Number.MIN_VALUE;\n    let index = -1;\n\n    for (let j = 0; j < numClasses; j++) {\n      if (scores[i * numClasses + j] > max) {\n        max = scores[i * numClasses + j];\n        index = j;\n      }\n    }\n\n    maxes[i] = max;\n    classes[i] = index;\n  }\n\n  return [maxes, classes];\n};\n\nconst buildDetectedObjects = (width, height, boxes, scores, indexes, classes, labels) => {\n  const count = indexes.length;\n  const objects = [];\n\n  for (let i = 0; i < count; i++) {\n    const bbox = [];\n\n    for (let j = 0; j < 4; j++) {\n      bbox[j] = boxes[indexes[i] * 4 + j];\n    }\n\n    const minY = bbox[0] * height;\n    const minX = bbox[1] * width;\n    const maxY = bbox[2] * height;\n    const maxX = bbox[3] * width;\n    bbox[0] = minX;\n    bbox[1] = minY;\n    bbox[2] = maxX - minX;\n    bbox[3] = maxY - minY;\n    objects.push({\n      bbox: bbox,\n      class: labels[parseInt(classes[indexes[i]])],\n      score: scores[indexes[i]]\n    });\n  }\n\n  return objects;\n};\n\nconst runPrediction = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(graph, labels, input) {\n    var batched, height, width, result, scores, boxes, _calculateMaxScores, _calculateMaxScores2, maxScores, classes, prevBackend, indexTensor, indexes;\n\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) switch (_context.prev = _context.next) {\n        case 0:\n          batched = tf.tidy(() => {\n            const img = tf.browser.fromPixels(input); // Reshape to a single-element batch so we can pass it to executeAsync.\n\n            return img.expandDims(0);\n          });\n          height = batched.shape[1];\n          width = batched.shape[2];\n          _context.next = 5;\n          return graph.executeAsync(batched);\n\n        case 5:\n          result = _context.sent;\n          scores = result[0].dataSync();\n          boxes = result[1].dataSync(); // clean the webgl tensors\n\n          batched.dispose();\n          tf.dispose(result);\n          _calculateMaxScores = calculateMaxScores(scores, result[0].shape[1], result[0].shape[2]), _calculateMaxScores2 = _slicedToArray(_calculateMaxScores, 2), maxScores = _calculateMaxScores2[0], classes = _calculateMaxScores2[1];\n          prevBackend = tf.getBackend(); // run post process in cpu\n\n          tf.setBackend('cpu');\n          indexTensor = tf.tidy(() => {\n            const boxes2 = tf.tensor2d(boxes, [result[1].shape[1], result[1].shape[3]]);\n            return tf.image.nonMaxSuppression(boxes2, maxScores, 20, // maxNumBoxes\n            0.5, // iou_threshold\n            0.5 // score_threshold\n            );\n          });\n          indexes = indexTensor.dataSync();\n          indexTensor.dispose(); // restore previous backend\n\n          tf.setBackend(prevBackend);\n          return _context.abrupt(\"return\", buildDetectedObjects(width, height, boxes, maxScores, indexes, classes, labels));\n\n        case 18:\n        case \"end\":\n          return _context.stop();\n      }\n    }, _callee);\n  }));\n\n  return function runPrediction(_x, _x2, _x3) {\n    return _ref.apply(this, arguments);\n  };\n}();\n\nexport default {\n  load: function () {\n    var _load = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(path) {\n      var graphPath, labelsPath, graphPromise, labelsPromise, _yield$Promise$all, _yield$Promise$all2, graph, labels;\n\n      return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n        while (1) switch (_context3.prev = _context3.next) {\n          case 0:\n            graphPath = path + '/model.json';\n            labelsPath = path + '/labels.json';\n            graphPromise = tf.loadGraphModel(graphPath);\n            labelsPromise = fetch(labelsPath).then(data => data.json());\n            _context3.next = 6;\n            return Promise.all([graphPromise, labelsPromise]);\n\n          case 6:\n            _yield$Promise$all = _context3.sent;\n            _yield$Promise$all2 = _slicedToArray(_yield$Promise$all, 2);\n            graph = _yield$Promise$all2[0];\n            labels = _yield$Promise$all2[1];\n            return _context3.abrupt(\"return\", {\n              detect: function () {\n                var _detect = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(input) {\n                  return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n                    while (1) switch (_context2.prev = _context2.next) {\n                      case 0:\n                        _context2.next = 2;\n                        return runPrediction(graph, labels, input);\n\n                      case 2:\n                        return _context2.abrupt(\"return\", _context2.sent);\n\n                      case 3:\n                      case \"end\":\n                        return _context2.stop();\n                    }\n                  }, _callee2);\n                }));\n\n                function detect(_x5) {\n                  return _detect.apply(this, arguments);\n                }\n\n                return detect;\n              }()\n            });\n\n          case 11:\n          case \"end\":\n            return _context3.stop();\n        }\n      }, _callee3);\n    }));\n\n    function load(_x4) {\n      return _load.apply(this, arguments);\n    }\n\n    return load;\n  }()\n};","map":null,"metadata":{},"sourceType":"module"}